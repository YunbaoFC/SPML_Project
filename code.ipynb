{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c290b9",
   "metadata": {},
   "source": [
    "# SPML Project\n",
    "\n",
    "##### Using graph attention to learn semantic in point cloud 3D manmade structure and keep only useful/significant points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2161bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "import networkx as nx\n",
    "import open3d as o3d\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c761d",
   "metadata": {},
   "source": [
    "## 0. Load datasets + Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330d2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_semantic3d_txt(path):\n",
    "    data = np.loadtxt(path)\n",
    "    points = data[:, 0:3]      # x, y, z\n",
    "    features = data[:, 3:7]    # intensity, r, g, b\n",
    "    return points, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee2f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(path):\n",
    "    \"\"\"Charge le fichier de labels (un label par ligne)\"\"\"\n",
    "    labels = np.loadtxt(path, dtype=int)\n",
    "    return labels\n",
    "\n",
    "def filter_by_label(points, features, labels, target_label=5):\n",
    "    \"\"\"Filtre les points pour ne garder que ceux avec le label cible\"\"\"\n",
    "    mask = (labels == target_label)\n",
    "    filtered_points = points[mask]\n",
    "    filtered_features = features[mask]\n",
    "    filtered_labels = labels[mask]\n",
    "    \n",
    "    print(f\"  Filtered: {len(points)} -> {len(filtered_points)} points (label={target_label})\")\n",
    "    return filtered_points, filtered_features, filtered_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7672a0f9",
   "metadata": {},
   "source": [
    "## 1. Convert points cloud to graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db431ad1",
   "metadata": {},
   "source": [
    "Radius estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78df2bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_radius(points, k=1, scale=2.0, sample_size=10000):\n",
    "    \"\"\"Estime le rayon en utilisant seulement un échantillon\"\"\"\n",
    "    if len(points) > sample_size:\n",
    "        indices = np.random.choice(len(points), sample_size, replace=False)\n",
    "        sample = points[indices]\n",
    "    else:\n",
    "        sample = points\n",
    "    \n",
    "    tree = KDTree(sample)\n",
    "    dists, _ = tree.query(sample, k=k+1)\n",
    "    return np.mean(dists[:, k]) * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b3069",
   "metadata": {},
   "source": [
    "Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41cae09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_radius_graph_batched(points, radius, max_neighbors=None, batch_size=5000):\n",
    "    \"\"\"Construit le graphe par lots pour éviter les problèmes de mémoire\"\"\"\n",
    "    kdtree = KDTree(points)\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # Ajouter tous les nœuds\n",
    "    for i in range(len(points)):\n",
    "        graph.add_node(i, pos=points[i].tolist())\n",
    "    \n",
    "    print(f\"  Building graph with {len(points)} nodes...\")\n",
    "    \n",
    "    # Traiter par lots\n",
    "    num_batches = (len(points) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(points))\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"  Batch {batch_idx + 1}/{num_batches}\")\n",
    "        \n",
    "        # Pour chaque point du lot, trouver ses voisins\n",
    "        for i in range(start_idx, end_idx):\n",
    "            # Requête pour ce point uniquement\n",
    "            neighbors = kdtree.query_ball_point(points[i], radius)\n",
    "            \n",
    "            # Limiter le nombre de voisins si nécessaire\n",
    "            if max_neighbors is not None and len(neighbors) > max_neighbors + 1:\n",
    "                # Calculer distances et garder les k plus proches\n",
    "                dists = np.linalg.norm(points[neighbors] - points[i], axis=1)\n",
    "                closest = np.argpartition(dists, max_neighbors + 1)[:max_neighbors + 1]\n",
    "                neighbors = [neighbors[idx] for idx in closest]\n",
    "            \n",
    "            # Ajouter les arêtes\n",
    "            for j in neighbors:\n",
    "                if i < j:  # Éviter les doublons (i,j) et (j,i)\n",
    "                    dist = np.linalg.norm(points[i] - points[j])\n",
    "                    graph.add_edge(i, j, weight=dist)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafc222",
   "metadata": {},
   "source": [
    "Save graph in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f673f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(graph, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(graph, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed0ae0",
   "metadata": {},
   "source": [
    "Load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c7b5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4af2cd",
   "metadata": {},
   "source": [
    "Load files and convert it into graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db01008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(\n",
    "    file_list,\n",
    "    input_dir,\n",
    "    labels_dir,\n",
    "    output_dir,\n",
    "    target_label=5,\n",
    "    max_neighbors=10,\n",
    "    radius_scale=2.0,\n",
    "    batch_size=5000,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in file_list:\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        label_filename = filename.replace(\".txt\", \".labels\")\n",
    "        label_path = os.path.join(labels_dir, label_filename)\n",
    "        \n",
    "        output_path = os.path.join(\n",
    "            output_dir,\n",
    "            filename.replace(\".txt\", \".graph\")\n",
    "        )\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"[SKIP] {filename} (graph already exists)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"[PROCESS] {filename}\")\n",
    "        \n",
    "        # 1. Load data\n",
    "        points, features = load_semantic3d_txt(input_path)\n",
    "        print(f\"  Loaded {len(points)} points\")\n",
    "        \n",
    "        # 2. Load labels\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"  WARNING: Label file not found: {label_path}\")\n",
    "            print(f\"  Skipping {filename}\")\n",
    "            continue\n",
    "        \n",
    "        labels = load_labels(label_path)\n",
    "        \n",
    "        if len(labels) != len(points):\n",
    "            print(f\"  ERROR: Mismatch between points ({len(points)}) and labels ({len(labels)})\")\n",
    "            continue\n",
    "        \n",
    "        # 3. Filter by label\n",
    "        filtered_points, filtered_features, filtered_labels = filter_by_label(\n",
    "            points, features, labels, target_label=target_label\n",
    "        )\n",
    "        \n",
    "        if len(filtered_points) == 0:\n",
    "            print(f\"  WARNING: No points with label {target_label} found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # 4. Estimate radius (sur échantillon)\n",
    "        radius = estimate_radius(filtered_points, k=1, scale=radius_scale)\n",
    "        print(f\"  Estimated radius: {radius:.4f}\")\n",
    "        \n",
    "        # 5. Build graph with batches\n",
    "        graph = build_radius_graph_batched(\n",
    "            filtered_points,\n",
    "            radius=radius,\n",
    "            max_neighbors=max_neighbors,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        print(f\"  Graph built: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
    "        \n",
    "        # 6. Attach node features\n",
    "        for i in range(len(filtered_points)):\n",
    "            graph.nodes[i][\"feat\"] = np.concatenate(\n",
    "                [filtered_points[i], filtered_features[i]]\n",
    "            ).tolist()\n",
    "            # Optionnel: ajouter aussi le label au nœud\n",
    "            graph.nodes[i][\"label\"] = int(filtered_labels[i])\n",
    "        \n",
    "        # 7. Save graph\n",
    "        save_graph(graph, output_path)\n",
    "        print(f\"  Saved graph to {output_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3d1a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESS] StGallenCathedral_station6_rgb_intensity-reduced.txt\n",
      "  Loaded 14608690 points\n",
      "  Filtered: 14608690 -> 2156724 points (label=5)\n",
      "  Estimated radius: 0.6902\n",
      "  Building graph with 2156724 nodes...\n",
      "  Batch 1/432\n",
      "  Batch 11/432\n",
      "  Batch 21/432\n",
      "  Batch 31/432\n",
      "  Batch 41/432\n",
      "  Batch 51/432\n",
      "  Batch 61/432\n",
      "  Batch 71/432\n",
      "  Batch 81/432\n",
      "  Batch 91/432\n",
      "  Batch 101/432\n",
      "  Batch 111/432\n",
      "  Batch 121/432\n",
      "  Batch 131/432\n",
      "  Batch 141/432\n",
      "  Batch 151/432\n",
      "  Batch 161/432\n",
      "  Batch 171/432\n",
      "  Batch 181/432\n",
      "  Batch 191/432\n",
      "  Batch 201/432\n",
      "  Batch 211/432\n",
      "  Batch 221/432\n",
      "  Batch 231/432\n",
      "  Batch 241/432\n",
      "  Batch 251/432\n",
      "  Batch 261/432\n",
      "  Batch 271/432\n",
      "  Batch 281/432\n",
      "  Batch 291/432\n",
      "  Batch 301/432\n",
      "  Batch 311/432\n",
      "  Batch 321/432\n",
      "  Batch 331/432\n",
      "  Batch 341/432\n",
      "  Batch 351/432\n",
      "  Batch 361/432\n",
      "  Batch 371/432\n",
      "  Batch 381/432\n",
      "  Batch 391/432\n",
      "  Batch 401/432\n",
      "  Batch 411/432\n",
      "  Batch 421/432\n",
      "  Batch 431/432\n",
      "  Graph built: 2156724 nodes, 10774108 edges\n",
      "  Saved graph to dataset/graphs\\StGallenCathedral_station6_rgb_intensity-reduced.graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_list = [\n",
    "    #\"MarketplaceFeldkirch_Station4_rgb_intensity-reduced.txt\",\n",
    "    \"StGallenCathedral_station6_rgb_intensity-reduced.txt\",\n",
    "]\n",
    "\n",
    "process_files(\n",
    "    file_list=file_list,\n",
    "    input_dir=\"dataset/raw_txt\",\n",
    "    labels_dir=\"dataset/labels\",\n",
    "    output_dir=\"dataset/graphs\",\n",
    "    target_label=5,  # Le label à garder\n",
    "    max_neighbors=10,\n",
    "    radius_scale=2.0,\n",
    "    batch_size=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc921eea",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea09e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GAT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import the base class with all helper methods\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mGAT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefinitions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGAT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GATLayerImp3\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGATWithSpatial\u001b[39;00m(GATLayerImp3):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extend GATLayerImp3 with spatial encoding\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'GAT'"
     ]
    }
   ],
   "source": [
    "# Import the base class with all helper methods\n",
    "from GAT.models.definitions.GAT import GATLayerImp3\n",
    "\n",
    "class GATWithSpatial(GATLayerImp3):\n",
    "    \"\"\"Extend GATLayerImp3 with spatial encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, \n",
    "                 concat=True, activation=nn.ELU(), dropout_prob=0.6, \n",
    "                 add_skip_connection=True, bias=True):\n",
    "        \n",
    "        # Initialize parent class - gets you ALL the helper methods\n",
    "        super().__init__(\n",
    "            num_in_features, num_out_features, num_of_heads,\n",
    "            concat, activation, dropout_prob, add_skip_connection, bias\n",
    "        )\n",
    "        \n",
    "        # ADD YOUR SPATIAL COMPONENTS\n",
    "        self.spatial_weight = nn.Parameter(torch.ones(1))\n",
    "        self.spatial_mlp = nn.Linear(3, num_of_heads)\n",
    "        nn.init.xavier_uniform_(self.spatial_mlp.weight)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"Override forward to add spatial encoding\"\"\"\n",
    "        in_nodes_features, edge_index, pos = data  # ADD pos\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        \n",
    "        # Dropout on input (from parent)\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "        \n",
    "        # Feature projection (from parent)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(\n",
    "            -1, self.num_of_heads, self.num_out_features)\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)\n",
    "        \n",
    "        # Attention scores (from parent)\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "        \n",
    "        # Lift to edges (parent's method)\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = \\\n",
    "            self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        \n",
    "        # ORIGINAL GAT scores\n",
    "        scores_per_edge = scores_source_lifted + scores_target_lifted\n",
    "        \n",
    "        # YOUR ADDITION: Spatial encoding\n",
    "        row, col = edge_index\n",
    "        delta_pos = pos[col] - pos[row]\n",
    "        spatial_scores = self.spatial_mlp(delta_pos)\n",
    "        scores_per_edge = scores_per_edge + self.spatial_weight * spatial_scores\n",
    "        \n",
    "        # Apply activation\n",
    "        scores_per_edge = self.leakyReLU(scores_per_edge)\n",
    "        \n",
    "        # Compute attention (parent's method)\n",
    "        attentions_per_edge = self.neighborhood_aware_softmax(\n",
    "            scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "        \n",
    "        # Aggregate (parent's method)\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "        out_nodes_features = self.aggregate_neighbors(\n",
    "            nodes_features_proj_lifted_weighted, edge_index, \n",
    "            in_nodes_features, num_of_nodes)\n",
    "        \n",
    "        # Skip connections, concat, bias (parent's method)\n",
    "        out_nodes_features = self.skip_concat_bias(\n",
    "            attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "        \n",
    "        return (out_nodes_features, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cee0f0",
   "metadata": {},
   "source": [
    "# Test with dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe2aab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GATWithSpatial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create layer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[43mGATWithSpatial\u001b[49m(\n\u001b[0;32m      3\u001b[0m     num_in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# xyz\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     num_out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      5\u001b[0m     num_of_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Dummy data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# 100 points\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GATWithSpatial' is not defined"
     ]
    }
   ],
   "source": [
    "# Create layer\n",
    "layer = GATWithSpatial(\n",
    "    num_in_features=3,  # xyz\n",
    "    num_out_features=64,\n",
    "    num_of_heads=4\n",
    ")\n",
    "\n",
    "# Dummy data\n",
    "N = 100  # 100 points\n",
    "features = torch.randn(N, 3)\n",
    "pos = torch.randn(N, 3)\n",
    "edge_index = torch.randint(0, N, (2, 500))  # 500 edges\n",
    "\n",
    "# Forward pass\n",
    "data = (features, edge_index, pos)\n",
    "output, _ = layer(data)\n",
    "\n",
    "print(f\"Input shape: {features.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Should be [100, 256] (4 heads * 64 features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
